name: Deploy openDAQ SDK packages on AWS

on:
  workflow_dispatch:

  push:
    branches: [main]

env:
  cmake_preset: package
  package_path: ${{ github.workspace }}/build/_packages
  package_path_rel: build/_packages # WA for https://github.com/actions/checkout/issues/785
  s3bucket: bb-blueberry-sdk-releases
  s3path: releases/${{ github.ref_name }}/SDK
  s3docpath: releases/${{ github.ref_name }}/Specifications and documentation
  s3simulatorpath: releases/${{ github.ref_name }}/Simulator
  vm_name: device_simulator
  simulator_app_artifact: SimulatorApp
  simulator_package_artifact: SimulatorPackage

jobs:
  deploy_windows:
    runs-on: ${{ matrix.runner }}
    name: ${{ matrix.name }}

    concurrency:
      group: ${{ github.workflow }}-${{ github.ref }}-${{ matrix.name }}
      cancel-in-progress: true
    timeout-minutes: 180
    strategy:
      fail-fast: false
      matrix:
        # https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners
        include:
          - name: Windows VS 2022 x64 Release
            runner: windows-latest
            cmake_generator: "Visual Studio 17 2022"
            cmake_generator_platform: x64
            cmake_build_type: Release
            cmake_defines: -DOPENDAQ_MSVC_SINGLE_PROCESS_BUILD=ON
            cpack: NSIS
            python_version: 3.11

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python_version }}

      - name: Configure
        run: |
          mkdir build
          cd build
          cmake -G "${{ matrix.cmake_generator }}" -A ${{ matrix.cmake_generator_platform }} --preset ${{ env.cmake_preset }} ${{ matrix.cmake_defines }} -DCMAKE_BUILD_TYPE=${{ matrix.cmake_build_type }} ..

      - name: Build
        working-directory: build
        run: cmake --build . --config ${{ matrix.cmake_build_type }}

      - name: Package
        working-directory: build
        run: cpack -C ${{ matrix.cmake_build_type }} -G ${{ matrix.cpack }}

      - name: Python package
        working-directory: build/bin/${{ matrix.cmake_build_type }}
        run: tar -a -c -f ${{ env.package_path }}/opendaq-python${{ matrix.python_version }}-win64.zip *.dll opendaq.*.pyd

      - uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
       
      - name: Get short SHA
        if: ${{ github.ref }} == 'refs/heads/main'
        id: short-sha
        uses: benjlevesque/short-sha@v2.2
        
      - name: Rename package (main)
        if: ${{ github.ref }} == 'refs/heads/main'
        shell: pwsh
        working-directory: ${{ env.package_path }}
        run: |
           Get-ChildItem -Path .\* -Include *.exe | Rename-Item -NewName {$_.BaseName + '_${{ steps.short-sha.outputs.sha }}' + $_.Extension}
      
      - name: Upload package to S3
        if: github.event_name == 'push'
        working-directory: ${{ env.package_path }}
        run: |
          aws s3 rm s3://${{ env.s3bucket }}/${{ env.s3path }}/ --recursive --exclude '*' --include '*.exe'
          aws s3 cp . s3://${{ env.s3bucket }}/${{ env.s3path }}/ --recursive

      - name: Upload package
        uses: actions/upload-artifact@v3
        with:
          name: Package (${{ matrix.name }})
          path: ${{ env.package_path }}
          retention-days: 7

  deploy_linux:
    runs-on: ubuntu-latest
    name: ${{ matrix.name }}

    concurrency:
      group: ${{ github.workflow }}-${{ github.ref }}-${{ matrix.name }}
      cancel-in-progress: true
    timeout-minutes: 180
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: Ubuntu 20.04 gcc-10 Release
            image: ubuntu:20.04
            cmake_generator: Ninja
            cmake_build_type: Release
            cmake_defines: -DOPENDAQ_BUILD_DOCUMENTATION=ON -DDAQSIMULATOR_ENABLE_SIMULATOR_APP=ON
            build_documentation: true
            cpack: DEB
            apt_packages: g++-10
            cc: gcc-10
            cxx: g++-10
            python_version: 38

    container:
      image: ${{ matrix.image }}
      env:
        TZ: Europe/Berlin
        DEBIAN_FRONTEND: noninteractive
        CC: ${{ matrix.cc }}
        CXX: ${{ matrix.cxx }}

    steps:
      - name: Install basic dependencies
        run: |
          apt-get update
          apt-get install -y git openssh-client rename

      - name: Checkout
        uses: actions/checkout@v3

      - name: Install build dependencies
        run: |
          apt-get install -y --no-install-recommends ${{ matrix.apt_packages }} \
            awscli \
            lld ninja-build curl \
            python3-pip python3-dev \
            mono-runtime libmono-system-json-microsoft4.0-cil libmono-system-data4.0-cil \
            libx11-dev libxcursor-dev libxrandr-dev libgl-dev libudev-dev libfreetype6-dev

      - name: Install documentation dependencies
        if: matrix.build_documentation
        run: apt-get install -y --no-install-recommends doxygen graphviz wget zip

      - name: Install latest CMake
        run: pip install cmake

      - name: Configure
        run: |
          mkdir build
          cd build
          cmake -G ${{ matrix.cmake_generator }} --preset ${{ env.cmake_preset }} ${{ matrix.cmake_defines }} -DCMAKE_BUILD_TYPE=${{ matrix.cmake_build_type }} ..

      - name: Build
        working-directory: build
        run: cmake --build .

      - name: Package
        working-directory: build
        run: cpack -C ${{ matrix.cmake_build_type }} -G ${{ matrix.cpack }}

      - name: Python package
        working-directory: build/bin
        run: tar -czvf $GITHUB_WORKSPACE/${{ env.package_path_rel }}/opendaq-python${{ matrix.python_version }}-x86_64-linux-gnu.tar.gz lib*.so opendaq*.so
      
      - name: Get short SHA
        if: ${{ github.ref }} == 'refs/heads/main'
        id: short-sha
        uses: benjlevesque/short-sha@v2.2
      
      - name: Rename package (main)
        working-directory: ${{ env.package_path_rel }}
        if: ${{ github.ref }} == 'refs/heads/main'
        run: |
          rename 's/_amd64/-ubuntu20.04-x86_64_${{ steps.short-sha.outputs.sha }}/' *.deb
          rename 's/opendaq_/opendaq-/' *.deb
          
      - name: Rename package
        working-directory: ${{ env.package_path_rel }}
        if: ${{ github.ref }} != 'refs/heads/main'
        run: |
          rename 's/_amd64/-ubuntu20.04-x86_64/' *.deb
          rename 's/opendaq_/opendaq-/' *.deb

      - name: Read openDAQ version
        id: daq_version
        working-directory: .
        run: |
            DAQ_VER=$(cat opendaq_version)
            echo Head openDAQ version: $DAQ_VER
            echo "DAQ_VERSION=$DAQ_VER" >> "$GITHUB_OUTPUT"

      - name: Compress documentation
        if: matrix.build_documentation
        working-directory: build/doc_doxygen/html
        run: zip -r "../../cpp_api_reference.zip" .

      - uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Upload package to S3
        if: github.event_name == 'push'
        working-directory: ${{ env.package_path_rel }}
        run: |
          aws s3 rm s3://${{ env.s3bucket }}/${{ env.s3path }}/  --recursive --exclude '*' --include '*.deb'
          aws s3 cp . s3://${{ env.s3bucket }}/${{ env.s3path }}/ --recursive

      - name: Upload package
        uses: actions/upload-artifact@v3
        with:
          name: Package (${{ matrix.name }})
          path: ${{ env.package_path_rel }}
          retention-days: 7

      - name: Upload documentation to S3
        if: matrix.build_documentation && github.event_name == 'push'
        working-directory: build
        run: |
          aws s3 rm "s3://${{ env.s3bucket }}/${{ env.s3docpath }}/"  --recursive --exclude '*' --include 'cpp_api_reference.zip'
          aws s3 cp "cpp_api_reference.zip" "s3://${{ env.s3bucket }}/${{ env.s3docpath }}/"

      # We only need to deploy to one of the two sites, but an extra API call doesn't hurt,
      # so for simplicity reasons we just call both.
      - name: Call the API to deploy the documentation
        if: github.event_name == 'push'
        run: |
          wget ${{ secrets.DEPLOY_DOCUMENTATION_URL_AND_TOKEN }}
          wget ${{ secrets.DEPLOY_DEV_DOCUMENTATION_URL_AND_TOKEN }}

      - name: Upload API reference
        uses: actions/upload-artifact@v3
        with:
          name: API Reference
          path: "build/cpp_api_reference.zip"
          retention-days: 7

      - name: Upload examples to S3
        if: github.event_name == 'push'
        working-directory: examples
        run: |
          zip -r opendaq-${{ steps.daq_version.outputs.DAQ_VERSION }}_examples.zip cpp python
          aws s3 rm s3://${{ env.s3bucket }}/${{ env.s3path }}/  --recursive --exclude '*' --include '*examples.zip'
          aws s3 cp opendaq-${{ steps.daq_version.outputs.DAQ_VERSION }}_examples.zip s3://${{ env.s3bucket }}/${{ env.s3path }}/

      - name: Upload temporary package for simulator
        uses: actions/upload-artifact@v3
        with:
          name: ${{ env.simulator_package_artifact }}
          path: ${{ env.package_path_rel }}

      - name: Upload simulator app
        uses: actions/upload-artifact@v3
        with:
          name: ${{ env.simulator_app_artifact }}
          path: build/bin/application_simulator

  simulator-build:
    runs-on: ubuntu-latest-4-cores
    name: Simulator build
    needs: [deploy_linux]

    concurrency:
      group: ${{ github.workflow }}-${{ github.ref }}-${{ matrix.name }}
      cancel-in-progress: true
    timeout-minutes: 120
    env:
      simulator_directory: simulator

    defaults:
      run:
        working-directory: ${{ env.simulator_directory }}

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y virtualbox vagrant

      - name: Download package
        uses: actions/download-artifact@v3
        with:
          name: ${{ env.simulator_package_artifact }}
          path: ${{ env.simulator_directory }}

      - name: Download simulator app
        uses: actions/download-artifact@v3
        with:
          name: ${{ env.simulator_app_artifact }}
          path: ${{ env.simulator_directory }}

      - name: Display files in working directory
        run: ls -Rhl

      - name: Read openDAQ version
        id: daq_version
        working-directory: .
        run: |
            DAQ_VER=$(cat opendaq_version)
            echo Head openDAQ version: $DAQ_VER
            echo "DAQ_VERSION=$DAQ_VER" >> "$GITHUB_OUTPUT"

      - name: Get short SHA
        if: ${{ github.ref }} == 'refs/heads/main'
        id: short-sha
        uses: benjlevesque/short-sha@v2.2
          
      - name: Define simulator name
        id: simulator_name
        run: |
          if [ "${{ github.ref }}" == "refs/heads/main" ];
          then
            echo "SIMULATOR_NAME=opendaq-${{ steps.daq_version.outputs.DAQ_VERSION }}_${{ env.vm_name }}_${{ steps.short-sha.outputs.sha }}" >> "$GITHUB_OUTPUT"
          else
            echo "SIMULATOR_NAME=opendaq-${{ steps.daq_version.outputs.DAQ_VERSION }}_${{ env.vm_name }}" >> "$GITHUB_OUTPUT"
          fi

      - name: Find binaries and run vagrant
        run: |
          debfiles=( *.deb )
          vagrant --version
          VM_NAME=${{ steps.simulator_name.outputs.SIMULATOR_NAME }} HOME_PATH=$HOME PACKAGE_NAME=${debfiles[0]} vagrant up
          VM_NAME=${{ steps.simulator_name.outputs.SIMULATOR_NAME }} HOME_PATH=$HOME PACKAGE_NAME=${debfiles[0]} vagrant halt

      - name: vboxmanage modify and compact
        run: |
          vboxmanage modifyvm ${{ steps.simulator_name.outputs.SIMULATOR_NAME }} --audio none --uart1 off --nic1 hostonly --hostonlyadapter1 "VirtualBox Host-Only Ethernet Adapter"
          vboxmanage modifymedium disk "${{ steps.simulator_name.outputs.SIMULATOR_NAME }}.vdi" --compact

      - name: vboxmanage check
        run: |
          vboxmanage list vms
          vboxmanage list hostonlyifs
          vboxmanage list hdds

      - name: export VM to .ova appliance
        run: |
          vboxmanage export ${{ steps.simulator_name.outputs.SIMULATOR_NAME }} -o ${{ steps.simulator_name.outputs.SIMULATOR_NAME }}.ova --options manifest,nomacs
          ls -hl ${{ steps.simulator_name.outputs.SIMULATOR_NAME }}.ova

      - uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Upload package to S3
        if: github.event_name == 'push'
        run: |
          aws s3 rm s3://${{ env.s3bucket }}/${{ env.s3simulatorpath }}/  --recursive --exclude '*' --include '*.ova'
          aws s3 cp ${{ steps.simulator_name.outputs.SIMULATOR_NAME }}.ova s3://${{ env.s3bucket }}/${{ env.s3simulatorpath }}/

      - name: Upload OVA image
        uses: actions/upload-artifact@v3
        with:
          name: ${{ steps.simulator_name.outputs.SIMULATOR_NAME }}-ova-image
          path: simulator/${{ steps.simulator_name.outputs.SIMULATOR_NAME }}.ova
          retention-days: 7

      - name: Delete unused artifacts
        uses: geekyeggo/delete-artifact@v2
        with:
          name: |
            ${{ env.simulator_app_artifact }}
            ${{ env.simulator_package_artifact }}

  build_documentation:
    runs-on: ubuntu-latest
    name: Build SDK Documentation
    timeout-minutes: 4
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Install Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'

      - name: Install Antora
        run: npm i antora

      - name: Install Antora extensions
        run: |
          npm i @asciidoctor/tabs
          npm i @springio/antora-extensions

      - name: Run Antora
        run: >
          [ "refs/heads/main" != "${{ github.ref }}" ] && npx antora antora-playbook.yml
          || (export DAQ_DEV_WEBSITE=docs-dev; npx antora antora-playbook-dev.yml)

      - name: Compress Documentation
        working-directory: ${{ github.workspace }}/build/site/
        run: zip -r ${{ github.workspace }}/build/user_guide.zip .

      - uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Upload Documentation to S3
        run: |
          aws s3 rm "s3://${{ env.s3bucket }}/${{ env.s3docpath }}/"  --recursive --exclude '*' --include 'user_guide.zip'
          aws s3 cp ${{ github.workspace }}/build/user_guide.zip "s3://${{ env.s3bucket }}/${{ env.s3docpath }}/"

      # We only need to deploy to one of the two sites, but an extra API call doesn't hurt,
      # so for simplicity reasons we just call both.
      - name: Call the API to deploy the documentation
        run: |
          wget ${{ secrets.DEPLOY_DOCUMENTATION_URL_AND_TOKEN }}
          wget ${{ secrets.DEPLOY_DEV_DOCUMENTATION_URL_AND_TOKEN }}

      - uses: actions/upload-artifact@v3
        with:
          name: openDAQ_SDK_User_Guide
          path: ${{ github.workspace }}/build/user_guide.zip
          retention-days: 7
